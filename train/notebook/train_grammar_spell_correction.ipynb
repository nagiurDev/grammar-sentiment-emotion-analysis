{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtRiDx59GuJe"
      },
      "outputs": [],
      "source": [
        "# 8ec29f3aeb5993c4d90cc69fcdf8682e2e550396"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch scikit-learn nltk"
      ],
      "metadata": {
        "id": "sIawX2FHIFAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Wk_cnOnEIFuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    lang8_df = pd.read_csv(\"preprocessed_lang8.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: preprocessed_lang8.csv not found. Make sure you have preprocessed the Lang-8 data.\")\n",
        "    # Exit or handle the error appropriately\n",
        "    # For example:\n",
        "    # import sys\n",
        "    # sys.exit(1)"
      ],
      "metadata": {
        "id": "jJJMUvs7IKoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_lang8 = DatasetDict({\n",
        "  'train': Dataset.from_pandas(lang8_df)\n",
        "})"
      ],
      "metadata": {
        "id": "DgjbAfizINgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")"
      ],
      "metadata": {
        "id": "svggdSpQIQz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    inputs = [\"grammar correction: \" + x for x in examples[\"text\"]] # Add special tokens for grammar correction.\n",
        "    targets = [x for x in examples[\"corrected_text\"]] # Set target values.\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs, text_target=targets, max_length=128, truncation=True, padding=True,\n",
        "    )\n",
        "\n",
        "    # Setup the tokenizer for targets. Add labels.\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, truncation=True, padding=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_lang8 = dataset_lang8.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "DfwgnEMfITd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
      ],
      "metadata": {
        "id": "bzrf1Oq8IW7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./grammar_correction_results\",\n",
        "    per_device_train_batch_size=8,  # Adjust batch size\n",
        "    per_device_eval_batch_size=8, # Adjust batch size\n",
        "    num_train_epochs=5,        # Adjust epochs\n",
        "    predict_with_generate=True,  # Required for text generation\n",
        "    eval_strategy = \"epoch\", # Evaluate every epoch\n",
        "    save_strategy=\"epoch\",     # Save after every epoch\n",
        "    load_best_model_at_end=True, # Load best model at end.\n",
        "    metric_for_best_model=\"bleu\", #Use bleu to measure performance.\n",
        "    # ... other training arguments (learning rate, warmup steps, etc.)\n",
        ")"
      ],
      "metadata": {
        "id": "18jXTB0oIY-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ],
      "metadata": {
        "id": "VYTwGzt4IiM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "PkqEp9J2ImwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_lang8[\"train\"], # Access the train split of the tokenized data.\n",
        "    eval_dataset=tokenized_lang8[\"validation\"],  # Use the appropriate split for evaluation.\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "nB8xeXxvItf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "2p70Ysc3JMA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_checkpoint = trainer.state.best_model_checkpoint\n",
        "best_model = AutoModelForSeq2SeqLM.from_pretrained(best_model_checkpoint)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "best_model.to(device)"
      ],
      "metadata": {
        "id": "SLqyuf6vJT0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_in_batches_seq2seq(model, dataset, batch_size=8):\n",
        "    all_predictions = []\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        batch = dataset[i : i + batch_size]\n",
        "        input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n",
        "        attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n",
        "        batch_input = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(**batch_input, max_length=128)  # Adjust max_length as needed\n",
        "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        all_predictions.extend(decoded_preds) #Extend, not append.\n",
        "\n",
        "    return all_predictions"
      ],
      "metadata": {
        "id": "YyA1omEGKHF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict_in_batches_seq2seq(best_model, tokenized_lang8[\"test\"])\n",
        "\n",
        "# Extract the true labels from the TEST set. They need to be decoded as well to enable bleu score calculation.\n",
        "labels = np.where(tokenized_lang8['test']['labels'] != -100, tokenized_lang8['test']['labels'], tokenizer.pad_token_id)\n",
        "true_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "#Evaluate using the metric\n",
        "test_metrics = metric.compute(predictions=predictions, references=[[label] for label in true_labels])\n",
        "\n",
        "#Print the bleu score.\n",
        "print(f\"Bleu score: {test_metrics['score']}\")"
      ],
      "metadata": {
        "id": "jetCPoXHKLPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EtqDZg0cK7tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.save_pretrained(\"./grammar_model\")\n",
        "torch.save(best_model.state_dict(), \"./cola_best_bert_model.pt\")"
      ],
      "metadata": {
        "id": "aCp33-26LLos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_sentence(sentence, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Corrects a single sentence using the fine-tuned grammar correction model.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The sentence to correct.\n",
        "        model: The fine-tuned grammar correction model.\n",
        "        tokenizer: The tokenizer used for the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The corrected sentence.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    inputs = tokenizer(\"grammar correction: \" + sentence, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)  # Add the special tokens\n",
        "    with torch.no_grad():\n",
        "        generated_tokens = model.generate(**inputs, max_length=128) # Adjust the max_length if necessary\n",
        "    corrected_sentence = tokenizer.decode(generated_tokens[0], skip_special_tokens=True) # Decode the first generated sentence\n",
        "    return corrected_sentence\n",
        "\n",
        "\n",
        "\n",
        "# Example usage (after loading your best model and tokenizer)\n",
        "example_sentence = \"I am go to the store yesterday.\"  # Example incorrect sentence\n",
        "corrected_sentence = predict_single_sentence(example_sentence, best_model, tokenizer)\n",
        "print(f\"Original: {example_sentence}\")\n",
        "print(f\"Corrected: {corrected_sentence}\")"
      ],
      "metadata": {
        "id": "Yub1u3GaNJR5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}