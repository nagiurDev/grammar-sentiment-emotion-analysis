# -*- coding: utf-8 -*-
"""train_grammar_correction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19urGw75WGfzhzOD_IElX8GM3KAQyDGZv

# Train `Grammar` Model

## Setup and Data Loading
"""

!pip install transformers datasets torch scikit-learn nltk evaluate

!pip install sacrebleu

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq
from datasets import load_dataset, DatasetDict, Dataset
from evaluate import load

import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from nltk.translate.bleu_score import corpus_bleu
import pandas as pd

try:
    lang8_df = pd.read_csv("lang8.csv")
except FileNotFoundError:
    print("Error: lang8.csv not found. Make sure you have preprocessed the Lang-8 data.")
    # Exit or handle the error appropriately
    # For example:
    # import sys
    # sys.exit(1)

lang8_df = lang8_df.head(50000)

len(lang8_df)

# 80% train, 20% for val and test
train_df, temp_df = train_test_split(lang8_df, test_size=0.2, random_state=42)

# 2. Split temp into val and test (e.g., 50/50 split of the remaining 20%)
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

print(f"Train size: {len(train_df)}")
print(f"Validation size: {len(val_df)}")
print(f"Test size: {len(test_df)}")

dataset_lang8 = DatasetDict({
  'train': Dataset.from_pandas(lang8_df),
  'validation': Dataset.from_pandas(val_df),
  'test': Dataset.from_pandas(test_df)
})



"""## Tokenization"""

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")

def tokenize_function(examples):
    inputs = ["grammar correction: " + x for x in examples["text"]]
    targets = [x for x in examples["corrected_text"]]

    model_inputs = tokenizer(
        inputs, text_target=targets, max_length=128, truncation=True, padding=True,
    )

    # Setup the tokenizer for targets. Add labels.
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=128, truncation=True, padding=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_lang8 = dataset_lang8.map(tokenize_function, batched=True)

tokenized_lang8

# tokenized_lang8['train']['labels'][0]

"""## Model Definition and Training Arguments"""

model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

training_args = Seq2SeqTrainingArguments(
    output_dir="./grammar_correction_results",
    per_device_train_batch_size=16,  # Adjust batch size
    per_device_eval_batch_size=32, # Adjust batch size
    num_train_epochs=3,        # Adjust epochs
    predict_with_generate=True,  # Required for text generation
    eval_strategy = "epoch", # Evaluate every epoch
    save_strategy="epoch",     # Save after every epoch
    load_best_model_at_end=True, # Load best model at end.
    metric_for_best_model="bleu", #Use bleu to measure performance.
)

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]


    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    result = {"bleu": result["score"]}

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result["gen_len"] = np.mean(prediction_lens)
    result = {k: round(v, 4) for k, v in result.items()}
    return result

metric = load("sacrebleu")

"""## Trainer and Training Loop"""

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_lang8["train"], # Access the train split of the tokenized data.
    eval_dataset=tokenized_lang8["validation"],  # Use the appropriate split for evaluation.
    compute_metrics=compute_metrics,
    data_collator=data_collator,
)

trainer.train()

"""## Evaluation"""

best_model_checkpoint = trainer.state.best_model_checkpoint
best_model = AutoModelForSeq2SeqLM.from_pretrained(best_model_checkpoint)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
best_model.to(device)

def predict_in_batches_seq2seq(model, dataset, batch_size=8):
    all_predictions = []
    for i in range(0, len(dataset), batch_size):
        batch = dataset[i : i + batch_size]
        input_ids = torch.tensor(batch["input_ids"]).to(device)
        attention_mask = torch.tensor(batch["attention_mask"]).to(device)
        batch_input = {"input_ids": input_ids, "attention_mask": attention_mask}
        with torch.no_grad():
            generated_tokens = model.generate(**batch_input, max_length=128)  # Adjust max_length as needed
            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

        all_predictions.extend(decoded_preds) #Extend, not append.

    return all_predictions

predictions = predict_in_batches_seq2seq(best_model, tokenized_lang8["test"])

# Extract the true labels from the TEST set. They need to be decoded as well to enable bleu score calculation.
labels = np.where(tokenized_lang8['test']['labels'] != -100, tokenized_lang8['test']['labels'], tokenizer.pad_token_id)
true_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)


#Evaluate using the metric
test_metrics = metric.compute(predictions=predictions, references=[[label] for label in true_labels])

#Print the bleu score.
print(f"Bleu score: {test_metrics['score']}")

# ... (previous code, including loading best_model and tokenized_test_lang8)

# 1. Generate Predictions (using model.generate, NOT just model(**inputs))
import torch

def predict_in_batches_seq2seq(model, dataset, batch_size=16):
    all_predictions = []
    for i in range(0, len(dataset), batch_size):
        batch = dataset[i : i + batch_size]
        input_ids = torch.tensor(batch["input_ids"]).to(device)
        attention_mask = torch.tensor(batch["attention_mask"]).to(device)
        batch_input = {"input_ids": input_ids, "attention_mask": attention_mask}
        with torch.no_grad():
            generated_ids = model.generate(**batch_input, max_length=128)  # Generate token IDs
            #Move to CPU if calculation done on CPU.
            generated_ids = generated_ids.to("cpu")
        all_predictions.append(generated_ids)
    return torch.cat(all_predictions, dim=0).numpy()  # Concatenate and convert to NumPy


predictions = predict_in_batches_seq2seq(best_model,  tokenized_lang8["test"])




# 2. Decode Predictions
decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)



# 3. Extract and Decode True Labels (as before)
labels =  tokenized_lang8["test"]['labels']
decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)


# 4. Prepare References for BLEU Calculation (as before)
references = [[label.strip()] for label in decoded_labels]

# 5. Calculate and print BLEU score
test_metrics = metric.compute(predictions=decoded_preds, references=references)
print(f"Bleu score: {test_metrics['score']}")



# ... (Save metrics and model)

"""## Saving the Model"""

# best_model.save_pretrained("./grammar_model")
torch.save(best_model.state_dict(), "./grammar_model.pt")

"""## Single Example Prediction"""

def predict_single_sentence(sentence, model, tokenizer):
    """
    Corrects a single sentence using the fine-tuned grammar correction model.

    Args:
        sentence (str): The sentence to correct.
        model: The fine-tuned grammar correction model.
        tokenizer: The tokenizer used for the model.

    Returns:
        str: The corrected sentence.
    """
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    inputs = tokenizer("grammar correction: " + sentence, padding="max_length", truncation=True, return_tensors="pt").to(device)  # Add the special tokens
    with torch.no_grad():
        generated_tokens = model.generate(**inputs, max_length=128) # Adjust the max_length if necessary
    corrected_sentence = tokenizer.decode(generated_tokens[0], skip_special_tokens=True) # Decode the first generated sentence
    return corrected_sentence



# Example usage (after loading your best model and tokenizer)
example_sentence = "I am go to the store yesterday."  # Example incorrect sentence
corrected_sentence = predict_single_sentence(example_sentence, best_model, tokenizer)
print(f"Original: {example_sentence}")
print(f"Corrected: {corrected_sentence}")

"""Temp

"""

from nltk.translate.bleu_score import corpus_bleu

# # Load preprocessed Lang-8 data
# try:
#     lang8_df = pd.read_csv("lang8.csv")
# except FileNotFoundError:
#     print("Error: preprocessed_lang8.csv not found. Make sure you have preprocessed the Lang-8 data.")
#     exit(1)  # Or handle the error as needed


# Split into train and validation (if you haven't already) - IMPORTANT!
train_df = lang8_df[:3000]  # Example: use the first 1000 examples for demonstration
validation_df = lang8_df[3000:3500] # Example: use examples 1000-1050 for demonstration. Change this index if you want to use more values.


# Calculate the identity baseline BLEU score on the VALIDATION set
references = [[text.split()] for text in validation_df['corrected_text']]
candidates = [text.split() for text in validation_df['text']]

baseline_bleu = corpus_bleu(references, candidates)
print(f"Lang-8 Identity Baseline BLEU: {baseline_bleu:.4f}")



def save_metrics_to_csv(metrics, filename="baseline_metrics.csv"):
    try:
        metrics_df = pd.DataFrame([metrics])
        metrics_df.to_csv(filename, index=False)
        print(f"Metrics saved to {filename}")
    except Exception as e:
        print(f"Error saving metrics: {e}")

# Example of saving the baseline BLEU score
baseline_metrics = {"bleu": baseline_bleu}
save_metrics_to_csv(baseline_metrics, filename="lang8_baseline_metrics.csv") # Save baseline bleu score.